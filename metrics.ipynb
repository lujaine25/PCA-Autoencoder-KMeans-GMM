{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d607c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842abbd",
   "metadata": {},
   "source": [
    "### Silhouette Score (manual implementation)\n",
    "\n",
    "This function computes the **average Silhouette Score** without sklearn.\n",
    "\n",
    "For each point:\n",
    "\n",
    "- **a(i)** → mean distance to points in the **same** cluster  \n",
    "- **b(i)** → mean distance to points in the **nearest other** cluster  \n",
    "\n",
    "Silhouette value:\n",
    "\n",
    "$$\n",
    "s(i)=\\frac{b(i)-a(i)}{\\max(a(i),\\, b(i))}\n",
    "$$\n",
    "\n",
    "Then it returns the **average of all s(i)**.\n",
    "\n",
    "Edge cases handled:\n",
    "- single-point clusters → score = 0  \n",
    "- only one cluster → score = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_score_manual(X, labels):\n",
    "    \"\"\"\n",
    "    Computes the mean Silhouette Coefficient of all samples.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    unique_labels = np.unique(labels) #the labels are the cluster assignments produced by your clustering algorithm.\n",
    "    s_values = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        point = X[i]\n",
    "        label = labels[i] # cluster assignment of the point\n",
    "        \n",
    "        # Calculate a(i): Mean distance to other points in the SAME cluster\n",
    "        same_cluster_mask = (labels == label)\n",
    "        # Exclude the point itself\n",
    "        other_points_indices = np.where(same_cluster_mask)[0] \n",
    "        other_points_indices = other_points_indices[other_points_indices != i] # Exclude self\n",
    "        \n",
    "        if len(other_points_indices) == 0: # Only one point in cluster\n",
    "            s_values.append(0) # Silhouette is undefined, assign 0\n",
    "            continue\n",
    "            \n",
    "        a_i = np.mean([euclidean_dist(point, X[idx]) for idx in other_points_indices])\n",
    "        \n",
    "        # Calculate b(i): Mean distance to points in the NEAREST neighboring cluster\n",
    "        b_i = np.inf\n",
    "        for other_label in unique_labels:\n",
    "            if other_label == label:\n",
    "                continue\n",
    "            \n",
    "            other_cluster_points = X[labels == other_label]\n",
    "            if len(other_cluster_points) == 0: # No points in this cluster\n",
    "                continue\n",
    "                \n",
    "            mean_dist = np.mean([euclidean_dist(point, p) for p in other_cluster_points])\n",
    "            b_i = min(b_i, mean_dist)\n",
    "        \n",
    "        # Silhouette for this point\n",
    "        if b_i == np.inf: # Only one cluster exists\n",
    "            s = 0\n",
    "        else:\n",
    "            s = (b_i - a_i) / max(a_i, b_i)\n",
    "        s_values.append(s)\n",
    "\n",
    "    return np.mean(s_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba0b73",
   "metadata": {},
   "source": [
    "### Davies–Bouldin Index (manual implementation)\n",
    "\n",
    "This function computes the **Davies–Bouldin Index (DBI)** — a clustering metric where  \n",
    "**lower values indicate better clustering**.\n",
    "\n",
    "For each cluster:\n",
    "\n",
    "1️-Compute the **centroid**  \n",
    "2️-Compute its **scatter** = average distance of points to the centroid  \n",
    "\n",
    "Then, for every pair of clusters \\(i,j\\):\n",
    "\n",
    "- compute centroid distance \\(M_{ij}\\)\n",
    "- compute similarity ratio:\n",
    "\n",
    "$$\n",
    "R_{ij} = \\frac{S_i + S_j}{M_{ij}}\n",
    "$$\n",
    "\n",
    "For each cluster, take the **maximum \\(R_{ij}\\)** (worst overlap), then average:\n",
    "\n",
    "$$\n",
    "DBI = \\frac{1}{k}\\sum_{i=1}^{k}\\max_{j\\ne i} R_{ij}\n",
    "$$\n",
    "\n",
    "**Lower DBI = tighter, better-separated clusters.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd852bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def davies_bouldin_manual(X, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels)\n",
    "    \n",
    "    #  Calculate Centroids and Scatter (Avg dist to centroid)\n",
    "    centroids = []\n",
    "    scatters = []\n",
    "    \n",
    "    for k in unique_labels:\n",
    "        cluster_points = X[labels == k]\n",
    "        centroid = np.mean(cluster_points, axis=0) #axis=0 for mean of each feature\n",
    "        centroids.append(centroid)\n",
    "        \n",
    "        # Average distance from points to centroid\n",
    "        avg_dist = np.mean([euclidean_dist(p, centroid) for p in cluster_points])\n",
    "        scatters.append(avg_dist)\n",
    "        \n",
    "    # Compute DB Index\n",
    "    db_score = 0\n",
    "    for i in range(n_clusters):\n",
    "        max_ratio = -np.inf\n",
    "        for j in range(n_clusters):\n",
    "            if i == j: # skip same cluster\n",
    "                continue\n",
    "            \n",
    "            # Distance between centroids\n",
    "            dist_centroids = euclidean_dist(centroids[i], centroids[j])\n",
    "            \n",
    "            # Ratio: (Scatter_i + Scatter_j) / Distance_ij\n",
    "            ratio = (scatters[i] + scatters[j]) / dist_centroids\n",
    "            \n",
    "            if ratio > max_ratio:\n",
    "                max_ratio = ratio\n",
    "        \n",
    "        db_score += max_ratio\n",
    "        \n",
    "    return db_score / n_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11642cc",
   "metadata": {},
   "source": [
    "### Calinski–Harabasz Index (manual implementation)\n",
    "\n",
    "This function computes the **Calinski–Harabasz Index (CHI)**, also called the *Variance Ratio Criterion*.  \n",
    "It compares **between-cluster dispersion** to **within-cluster dispersion**.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Compute the **global mean** of all data points.  \n",
    "2. For each cluster:\n",
    "   - compute its **centroid**\n",
    "   - accumulate:\n",
    "     - \\(W_k\\): sum of squared distances of points to their centroid  \n",
    "     - \\(B_k\\): weighted squared distance of the centroid to the global mean\n",
    "\n",
    "The index is:\n",
    "\n",
    "$$\n",
    "CHI = \\frac{B_k / (k-1)}{W_k / (n-k)}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "**Higher CHI indicates tighter clusters that are well separated.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calinski_harabasz_manual(X, labels):\n",
    "    n_samples = X.shape[0]\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels)\n",
    "    \n",
    "    if n_clusters < 2:\n",
    "        return 0\n",
    "        \n",
    "    global_mean = np.mean(X, axis=0)\n",
    "    \n",
    "    # Within-Cluster Scatter (W_k) & Between-Cluster Scatter (B_k)\n",
    "    W_k = 0\n",
    "    B_k = 0\n",
    "    \n",
    "    for k in unique_labels: #calculate for each cluster\n",
    "        cluster_points = X[labels == k]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        n_points = len(cluster_points)\n",
    "        \n",
    "        # W_k contribution: Sum of squared distances to centroid\n",
    "        dists_to_centroid = np.sum((cluster_points - centroid)**2)\n",
    "        W_k += dists_to_centroid\n",
    "        \n",
    "        # B_k contribution: Weighted squared distance of centroid to global mean\n",
    "        B_k += n_points * np.sum((centroid - global_mean)**2)  #sum for each feature and weight by number of points\n",
    "        \n",
    "    # Formula: (B_k / (k - 1)) / (W_k / (n - k))\n",
    "    ch_index = (B_k / (n_clusters - 1)) / (W_k / (n_samples - n_clusters))\n",
    "    \n",
    "    return ch_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2f03f",
   "metadata": {},
   "source": [
    "### WCSS (Within-Cluster Sum of Squares) and GMM Metrics\n",
    "\n",
    "**WCSS / Inertia**  \n",
    "Measures cluster compactness for k-means:\n",
    "\n",
    "- For each point, compute squared distance to its cluster centroid  \n",
    "- Sum over all points:\n",
    "\n",
    "$$\n",
    "WCSS = \\sum_{i=1}^{n} \\| x_i - \\mu_{c_i} \\|^2\n",
    "$$\n",
    "\n",
    "Lower WCSS indicates tighter clusters.\n",
    "\n",
    "---\n",
    "\n",
    "**GMM Metrics (AIC & BIC)**  \n",
    "Used to evaluate Gaussian Mixture Models:\n",
    "\n",
    "- **AIC** (Akaike Information Criterion):\n",
    "\n",
    "$$\n",
    "AIC = 2k - 2 \\ln(L)\n",
    "$$\n",
    "\n",
    "- **BIC** (Bayesian Information Criterion):\n",
    "\n",
    "$$\n",
    "BIC = k \\ln(n) - 2 \\ln(L)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\(k\\) = number of parameters (means, covariances, weights)  \n",
    "- \\(L\\) = log-likelihood  \n",
    "- \\(n\\) = number of samples  \n",
    "\n",
    "Lower AIC/BIC indicates a better model, balancing fit and complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ccd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WCSS (Inertia) wcss-> within cluster sum of squares\n",
    "def compute_wcss(X, labels, centroids):\n",
    "    wcss = 0\n",
    "    for i, point in enumerate(X):\n",
    "        centroid = centroids[labels[i]]\n",
    "        wcss += np.sum((point - centroid)**2)\n",
    "    return wcss\n",
    "\n",
    "# GMM Metrics\n",
    "def gmm_metrics(log_likelihood, n_samples, n_features, n_clusters, covariance_type='full'):\n",
    "    \"\"\"\n",
    "    Calculates AIC and BIC for GMM.\n",
    "    \"\"\"\n",
    "    # Number of parameters calculation (approximate for 'full' covariance)\n",
    "    # Means (k*d) + Covariances (k * d*(d+1)/2) + Weights (k-1)\n",
    "    n_params = (n_clusters * n_features) + \\\n",
    "               (n_clusters * n_features * (n_features + 1) / 2) + \\\n",
    "               (n_clusters - 1)\n",
    "               \n",
    "    # BIC = k*ln(n) - 2*ln(L)\n",
    "    bic = (n_params * np.log(n_samples)) - (2 * log_likelihood)\n",
    "    \n",
    "    # AIC = 2k - 2*ln(L)\n",
    "    aic = (2 * n_params) - (2 * log_likelihood)\n",
    "    \n",
    "    return aic, bic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1890217",
   "metadata": {},
   "source": [
    "### Purity Score (manual implementation)\n",
    "\n",
    "Purity measures how **homogeneous clusters** are with respect to the true labels.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. For each predicted cluster:\n",
    "   - Find the **true labels** of points in that cluster  \n",
    "   - Identify the **most frequent true label**  \n",
    "   - Count points correctly assigned to this label\n",
    "\n",
    "2. Sum the counts over all clusters and divide by total samples:\n",
    "\n",
    "$$\n",
    "Purity = \\frac{1}{N} \\sum_{k} \\max_j | c_k \\cap t_j |\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\(c_k\\) = points in predicted cluster \\(k\\)  \n",
    "- \\(t_j\\) = points of true class \\(j\\)  \n",
    "- \\(N\\) = total number of points\n",
    "\n",
    "Purity ranges from **0 to 1**, with **1 being perfectly pure clusters**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999e4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score_manual(y_true, y_pred):\n",
    "    # Compute confusion matrix first (intersection counts)\n",
    "    # Rows: True, Cols: Pred\n",
    "    unique_true = np.unique(y_true)\n",
    "    unique_pred = np.unique(y_pred)\n",
    "    \n",
    "    total_samples = len(y_true)\n",
    "    correct_counts = 0\n",
    "    \n",
    "    for p in unique_pred:\n",
    "        # Find indices where prediction is p\n",
    "        pred_mask = (y_pred == p)\n",
    "        \n",
    "        # Get true labels for these points\n",
    "        true_labels_in_cluster = y_true[pred_mask]\n",
    "        \n",
    "        if len(true_labels_in_cluster) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Find most frequent true label in this cluster\n",
    "        values, counts = np.unique(true_labels_in_cluster, return_counts=True)\n",
    "        max_count = np.max(counts)\n",
    "        \n",
    "        correct_counts += max_count\n",
    "        \n",
    "    return correct_counts / total_samples #true positive rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d28c8f",
   "metadata": {},
   "source": [
    "### Adjusted Rand Index (manual implementation)\n",
    "\n",
    "The **Adjusted Rand Index (ARI)** measures similarity between **true labels** and **predicted clusters**, correcting for chance.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Compute the **contingency table** between true classes and predicted clusters.  \n",
    "2. Count all pairs of points:\n",
    "\n",
    "- **nC2 for predicted clusters** → \\( \\text{tp\\_plus\\_fp} \\)  \n",
    "- **nC2 for true classes** → \\( \\text{tp\\_plus\\_fn} \\)  \n",
    "- **nC2 for intersections** → \\( \\text{tp} \\)\n",
    "\n",
    "3. Compute total pairs: \\( \\text{nC2(total samples)} \\)  \n",
    "4. Compute **expected index** (random chance) and **maximum index**  \n",
    "5. Apply ARI formula:\n",
    "\n",
    "$$\n",
    "ARI = \\frac{TP - \\text{Expected Index}}{\\text{Max Index} - \\text{Expected Index}}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- **1** → perfect agreement  \n",
    "- **0** → random labeling  \n",
    "- Can be negative → worse than random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ea4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb # Used only for nCr calculation\n",
    "\n",
    "def adjusted_rand_index_manual(y_true, y_pred):\n",
    "    # Helper: nC2 (combinations of 2)\n",
    "    def nC2(n):\n",
    "        if n < 2: return 0\n",
    "        return n * (n - 1) / 2\n",
    "\n",
    "    #  Contingency Table\n",
    "    classes = np.unique(y_true)\n",
    "    clusters = np.unique(y_pred)\n",
    "    \n",
    "    tp_plus_fp = 0 # Sum of nC2 for predicted clusters (a_i)\n",
    "    tp_plus_fn = 0 # Sum of nC2 for true classes (b_j)\n",
    "    tp = 0         # Sum of nC2 for intersection (n_ij)\n",
    "    \n",
    "    # Calculate row sums (a_i)\n",
    "    for c in clusters:\n",
    "        n_c = np.sum(y_pred == c)\n",
    "        tp_plus_fp += nC2(n_c)\n",
    "        \n",
    "    # Calculate col sums (b_j)\n",
    "    for k in classes:\n",
    "        n_k = np.sum(y_true == k)\n",
    "        tp_plus_fn += nC2(n_k)\n",
    "        \n",
    "    # Calculate intersections (n_ij)\n",
    "    for c in clusters:\n",
    "        for k in classes:\n",
    "            n_ij = np.sum((y_pred == c) & (y_true == k))\n",
    "            tp += nC2(n_ij)\n",
    "            \n",
    "    # Total pairs\n",
    "    n_samples = len(y_true)\n",
    "    total_pairs = nC2(n_samples)\n",
    "    \n",
    "    # Expected Index (by chance)\n",
    "    expected_index = (tp_plus_fp * tp_plus_fn) / total_pairs\n",
    "    \n",
    "    # Max Index\n",
    "    max_index = (tp_plus_fp + tp_plus_fn) / 2\n",
    "    \n",
    "    # ARI Formula\n",
    "    if max_index == expected_index:\n",
    "        return 0\n",
    "    return (tp - expected_index) / (max_index - expected_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd7fef",
   "metadata": {},
   "source": [
    "### Normalized Mutual Information (manual implementation)\n",
    "\n",
    "**NMI** measures the similarity between **true labels** and **predicted clusters**, normalized to [0,1].\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Compute **entropy** of true labels \\(H(Y)\\) and predicted clusters \\(H(C)\\):\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_i p_i \\log(p_i)\n",
    "$$\n",
    "\n",
    "2. Compute **Mutual Information**:\n",
    "\n",
    "$$\n",
    "I(Y;C) = \\sum_{y \\in Y} \\sum_{c \\in C} P(y,c) \\log \\frac{P(y,c)}{P(y) P(c)}\n",
    "$$\n",
    "\n",
    "3. Normalize:\n",
    "\n",
    "$$\n",
    "NMI = \\frac{2 \\cdot I(Y;C)}{H(Y) + H(C)}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- **1** → perfect agreement  \n",
    "- **0** → no mutual information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b66b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    \"\"\" Helper to compute Entropy H(X) \"\"\"\n",
    "    n = len(labels)\n",
    "    if n == 0: return 0\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    probs = counts / n\n",
    "    return -np.sum(probs * np.log(probs + 1e-10)) # 1e-10 for numerical stability\n",
    "\n",
    "def nmi_manual(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    \n",
    "    # Entropy of Class H(Y) and Cluster H(C)\n",
    "    H_Y = entropy(y_true)\n",
    "    H_C = entropy(y_pred)\n",
    "    \n",
    "    # Mutual Information I(Y; C)\n",
    "    # I(Y; C) = Sum_y Sum_c P(y,c) * log( P(y,c) / (P(y)*P(c)) )\n",
    "    MI = 0\n",
    "    classes = np.unique(y_true)\n",
    "    clusters = np.unique(y_pred)\n",
    "    \n",
    "    for k in classes:\n",
    "        for c in clusters:\n",
    "            # Intersection count\n",
    "            n_kc = np.sum((y_true == k) & (y_pred == c))\n",
    "            \n",
    "            if n_kc > 0:\n",
    "                p_kc = n_kc / n\n",
    "                p_k = np.sum(y_true == k) / n\n",
    "                p_c = np.sum(y_pred == c) / n\n",
    "                \n",
    "                MI += p_kc * np.log(p_kc / (p_k * p_c + 1e-10))\n",
    "                \n",
    "    # Normalized MI\n",
    "    # NMI = 2 * I(Y;C) / (H(Y) + H(C))\n",
    "    if (H_Y + H_C) == 0:\n",
    "        return 0\n",
    "    return 2 * MI / (H_Y + H_C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
